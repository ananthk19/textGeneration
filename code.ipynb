{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 19:52:09.600911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-13 19:52:13.380084: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:13.380162: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-13 19:52:18.002555: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:18.002653: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:18.002665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-13 19:52:29.353810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-13 19:52:29.354053: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354131: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354191: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354250: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354309: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354481: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-05-13 19:52:29.354491: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not (token.is_stop) ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def find_max_len_string(texts):\n",
    "    max_len = len(texts.loc[0])\n",
    "    max_string = ''\n",
    "    i = 0\n",
    "    while i < len(df1):\n",
    "        s = texts.loc[i]\n",
    "        # print(s)\n",
    "        if len(s) < max_len:\n",
    "            max_len = len(s)\n",
    "            max_string = s\n",
    "        i += 1\n",
    "    return  max_string\n",
    "\n",
    "def findMinMax(arrays):\n",
    "    max = None\n",
    "    max_len = 0\n",
    "    min_len = len(arrays[0])\n",
    "    min = arrays[0]\n",
    "    for array in arrays:\n",
    "        size = len(array)\n",
    "        if size > max_len:\n",
    "            max = array\n",
    "        if size < min_len:\n",
    "            min = array\n",
    "    return max, min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target          id                          date      flag             user  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                text  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Assignments/archive/training.1600000.processed.noemoticon.csv', encoding = \"cp1252\")\n",
    "first_row = df.columns\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "first_row_df = pd.DataFrame(data=None, columns = df.columns)\n",
    "first_row_df.loc[0] = first_row\n",
    "df = pd.concat([first_row_df,df])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vacant_heart' 'TizBanana' 'michxxblc' 'whitsundays' 'MandyPandy32'\n",
      " 'brianwelburn' 'roxiijonas' 'SarahSaner' 'original_one' 'kasey79'\n",
      " 'jesssicababesss' 'x33ieroNINJA' 'tallivansunder' 'ShesElectric_'\n",
      " 'JeremyCShipp' 'dontforgetchaos' 'semipenguin' 'BBRRIITTTTYY' 'caldjr'\n",
      " 'joshtastic1' 'baldeggie' 'DsBabyGirl' 'wyndwitch' 'Anime81'\n",
      " 'stevegarufi' 'Mirna023' 'heykim' 'keren4562' 'WalkingHorse'\n",
      " 'chiniehdiaz' 'NKANGEL74' 'radha_' 'Wolfgang_' 'AceMas21' 'ComedyQueen'\n",
      " 'sharonhayes' 'Kikirowr' 'NKArmyTNgirl' 'MsStaceyK' 'jonas_twilight3'\n",
      " 'WTFJAY' 'JennaMadison' 'jennypoynter' 'krist0ph3r' 'luckygnahhh'\n",
      " 'hypnophil' 'adlyman' 'funkyfairy24' 'BeckyKingston' 'kat_n'\n",
      " 'happylovesChuck' 'MCRmuffin' 'chantellmarie' 'gemeg' 'hummingbird604'\n",
      " 'longestpoem' 'CaraNinaMcfly' 'fadedmoon' 'annasaccone' 'Gen22'\n",
      " 'BrandyWandLover' 'figPYBFO' 'tweeteradder7' 'tweeteradder2' 'mp3mad'\n",
      " 'primatage' 'amysav83' 'aussiemcflyfan' 'Samm_xo' 'lopezwilfred'\n",
      " 'Hollywood_Trey' 'ladybug8320' 'TidyCat' 'QueenBxoxo' 'janine_j9'\n",
      " 'MyAppleStuff' '5toSucceed' 'pato_30stm' 'BlokesLib' 'wendywings'\n",
      " 'marco_cali' 'redrobinrockn' 'bryancheung' 'mlexiehayden' 'pdurham'\n",
      " 'feblub' 'effingcards' 'NathalieCaron' 'MissxMarisa' '19fischi75'\n",
      " 'danger_skies' 'SarahMorrison' 'Shinybiscuit' 'AnnSue' 'sprinkles_'\n",
      " 'sebby_peek' 'carole29' 'vintagepolka' 'endlessblush' 'hilzfuld'\n",
      " 'Leanne0710' 'twishes' 'C_Joy' 'flamingokitty' 'leewaters'\n",
      " 'iyaitssuzanne']\n"
     ]
    }
   ],
   "source": [
    "user_frequency = df.user.value_counts()\n",
    "preferred_users = user_frequency[user_frequency > 100]\n",
    "preferred_users = preferred_users[user_frequency < 150]\n",
    "print(preferred_users.index.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose user DarkPiano for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      @Eyglo how do u open these blip fm streaming f...\n",
       "1                 Damn! The Firefox poker aint loading! \n",
       "2                Damn! The facebook poker aint loading! \n",
       "3      @Richard_Gable good morning mate.Dark and clou...\n",
       "4      @Dr_DinaSadik forensic mein kya hai! bas rat t...\n",
       "                             ...                        \n",
       "142    @nessie_111 f1 today?Oh well the world cup's r...\n",
       "143    @daaku isnt tweetdeck a bit too huge for an in...\n",
       "144    btw hav found the perfect twitter client for m...\n",
       "145                                 @drsimonc  well said\n",
       "146    @jenna_rater quite agree. Was watching an afri...\n",
       "Name: text, Length: 147, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = 'vacant_heart'\n",
    "df1 = df[df.loc[:,\"user\"] == user]\n",
    "df1 = df1.reset_index()\n",
    "df1.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.text = df1.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Bass _ happen EU election btw ? mean u vote ? sorry , know EU election .\n"
     ]
    }
   ],
   "source": [
    "print(df1.text[32])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2303\n"
     ]
    }
   ],
   "source": [
    "seq_length = 30\n",
    "\n",
    "# Create a sequence of tokens\n",
    "sequences = []\n",
    "for i in range(len(df1)):\n",
    "    tweet = df1.text.loc[i]\n",
    "\n",
    "    for i in range(seq_length, len(tweet)):\n",
    "        seq = tweet[i-seq_length:i+1]\n",
    "        sequences.append(seq)\n",
    "\n",
    "# Convert sequences into numerical data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=seq_length+1, padding='pre', truncating='pre')\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "# Take the last integer of the arrays as y and rest as X\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3450, 30) (3450, 2303)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 30, 180)           414540    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 30, 250)           431000    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 250)               501000    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               75300     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2303)              693203    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,115,043\n",
      "Trainable params: 2,115,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(vocab_size, 180, input_length=seq_length),\n",
    "    layers.LSTM(250, return_sequences=True),\n",
    "    layers.LSTM(250),\n",
    "    layers.Dense(300, activation='relu'),\n",
    "    layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 14s 375ms/step - loss: 7.4338 - accuracy: 0.0122\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 10s 383ms/step - loss: 6.8888 - accuracy: 0.0145\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 11s 389ms/step - loss: 6.8004 - accuracy: 0.0136\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 10s 376ms/step - loss: 6.7732 - accuracy: 0.0116\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 10s 374ms/step - loss: 6.7516 - accuracy: 0.0139\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 6.6752 - accuracy: 0.0130\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 10s 382ms/step - loss: 6.6028 - accuracy: 0.0136\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 10s 387ms/step - loss: 6.5437 - accuracy: 0.0171\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 11s 389ms/step - loss: 6.4513 - accuracy: 0.0194\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 10s 373ms/step - loss: 6.2550 - accuracy: 0.0214\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 6.0198 - accuracy: 0.0223\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 5.7777 - accuracy: 0.0261\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 10s 372ms/step - loss: 5.5644 - accuracy: 0.0290\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 5.3629 - accuracy: 0.0394\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 10s 373ms/step - loss: 5.1503 - accuracy: 0.0472\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 10s 374ms/step - loss: 4.9300 - accuracy: 0.0655\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 10s 381ms/step - loss: 4.7294 - accuracy: 0.0759\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 10s 386ms/step - loss: 4.5431 - accuracy: 0.0991\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 11s 394ms/step - loss: 4.3723 - accuracy: 0.1136\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 11s 393ms/step - loss: 4.2252 - accuracy: 0.1197\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 4.0820 - accuracy: 0.1452\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 10s 373ms/step - loss: 3.9494 - accuracy: 0.1559\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 3.8207 - accuracy: 0.1725\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 3.7168 - accuracy: 0.1687\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 11s 415ms/step - loss: 3.5811 - accuracy: 0.2046\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 3.4609 - accuracy: 0.2183\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 3.3692 - accuracy: 0.2301\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 3.2657 - accuracy: 0.2377\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 11s 391ms/step - loss: 3.1507 - accuracy: 0.2600\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 11s 390ms/step - loss: 3.0460 - accuracy: 0.2757\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 11s 392ms/step - loss: 2.9496 - accuracy: 0.2893\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 11s 390ms/step - loss: 2.8727 - accuracy: 0.2896\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 11s 390ms/step - loss: 2.7595 - accuracy: 0.3145\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 2.6724 - accuracy: 0.3359\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 2.6057 - accuracy: 0.3377\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 2.5207 - accuracy: 0.3577\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 2.4415 - accuracy: 0.3623\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 2.3661 - accuracy: 0.3812\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 2.3131 - accuracy: 0.3916\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 11s 394ms/step - loss: 2.2360 - accuracy: 0.4020\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 2.1699 - accuracy: 0.4113\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 2.1425 - accuracy: 0.4075\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 2.0727 - accuracy: 0.4301\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 2.0020 - accuracy: 0.4446\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 1.9498 - accuracy: 0.4522\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 1.9030 - accuracy: 0.4629\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 11s 392ms/step - loss: 1.8687 - accuracy: 0.4612\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 1.8163 - accuracy: 0.4736\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 1.7739 - accuracy: 0.4890\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 1.7231 - accuracy: 0.4988\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 1.6921 - accuracy: 0.5061\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 1.6346 - accuracy: 0.5191\n",
      "Epoch 53/100\n",
      "27/27 [==============================] - 11s 394ms/step - loss: 1.5976 - accuracy: 0.5301\n",
      "Epoch 54/100\n",
      "27/27 [==============================] - 11s 414ms/step - loss: 1.5585 - accuracy: 0.5322\n",
      "Epoch 55/100\n",
      "27/27 [==============================] - 11s 406ms/step - loss: 1.5157 - accuracy: 0.5441\n",
      "Epoch 56/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 1.4671 - accuracy: 0.5597\n",
      "Epoch 57/100\n",
      "27/27 [==============================] - 11s 400ms/step - loss: 1.4358 - accuracy: 0.5696\n",
      "Epoch 58/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 1.4156 - accuracy: 0.5655\n",
      "Epoch 59/100\n",
      "27/27 [==============================] - 11s 400ms/step - loss: 1.3593 - accuracy: 0.5843\n",
      "Epoch 60/100\n",
      "27/27 [==============================] - 11s 402ms/step - loss: 1.3447 - accuracy: 0.5884\n",
      "Epoch 61/100\n",
      "27/27 [==============================] - 11s 403ms/step - loss: 1.2888 - accuracy: 0.5994\n",
      "Epoch 62/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 1.2799 - accuracy: 0.6003\n",
      "Epoch 63/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 1.2723 - accuracy: 0.6067\n",
      "Epoch 64/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 1.2389 - accuracy: 0.6171\n",
      "Epoch 65/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 1.1934 - accuracy: 0.6241\n",
      "Epoch 66/100\n",
      "27/27 [==============================] - 11s 400ms/step - loss: 1.1500 - accuracy: 0.6400\n",
      "Epoch 67/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 1.1288 - accuracy: 0.6484\n",
      "Epoch 68/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 1.1036 - accuracy: 0.6446\n",
      "Epoch 69/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 1.0761 - accuracy: 0.6533\n",
      "Epoch 70/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 1.0692 - accuracy: 0.6542\n",
      "Epoch 71/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 1.0662 - accuracy: 0.6568\n",
      "Epoch 72/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 1.0079 - accuracy: 0.6814\n",
      "Epoch 73/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 0.9896 - accuracy: 0.6800\n",
      "Epoch 74/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 0.9723 - accuracy: 0.6896\n",
      "Epoch 75/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 0.9428 - accuracy: 0.6988\n",
      "Epoch 76/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 0.9208 - accuracy: 0.7032\n",
      "Epoch 77/100\n",
      "27/27 [==============================] - 11s 394ms/step - loss: 0.8855 - accuracy: 0.7162\n",
      "Epoch 78/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 0.8762 - accuracy: 0.7209\n",
      "Epoch 79/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 0.8695 - accuracy: 0.7171\n",
      "Epoch 80/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 0.8682 - accuracy: 0.7142\n",
      "Epoch 81/100\n",
      "27/27 [==============================] - 13s 397ms/step - loss: 0.8369 - accuracy: 0.7310\n",
      "Epoch 82/100\n",
      "27/27 [==============================] - 11s 389ms/step - loss: 0.8089 - accuracy: 0.7293\n",
      "Epoch 83/100\n",
      "27/27 [==============================] - 11s 391ms/step - loss: 0.7938 - accuracy: 0.7383\n",
      "Epoch 84/100\n",
      "27/27 [==============================] - 11s 391ms/step - loss: 0.7721 - accuracy: 0.7461\n",
      "Epoch 85/100\n",
      "27/27 [==============================] - 11s 390ms/step - loss: 0.7678 - accuracy: 0.7496\n",
      "Epoch 86/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 0.7493 - accuracy: 0.7475\n",
      "Epoch 87/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 0.7358 - accuracy: 0.7507\n",
      "Epoch 88/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 0.7139 - accuracy: 0.7672\n",
      "Epoch 89/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 0.6912 - accuracy: 0.7733\n",
      "Epoch 90/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 0.6928 - accuracy: 0.7606\n",
      "Epoch 91/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 0.6768 - accuracy: 0.7722\n",
      "Epoch 92/100\n",
      "27/27 [==============================] - 11s 406ms/step - loss: 0.6645 - accuracy: 0.7768\n",
      "Epoch 93/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 0.6665 - accuracy: 0.7678\n",
      "Epoch 94/100\n",
      "27/27 [==============================] - 11s 394ms/step - loss: 0.6418 - accuracy: 0.7855\n",
      "Epoch 95/100\n",
      "27/27 [==============================] - 11s 395ms/step - loss: 0.6235 - accuracy: 0.7890\n",
      "Epoch 96/100\n",
      "27/27 [==============================] - 11s 399ms/step - loss: 0.6173 - accuracy: 0.7896\n",
      "Epoch 97/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 0.6024 - accuracy: 0.7893\n",
      "Epoch 98/100\n",
      "27/27 [==============================] - 11s 398ms/step - loss: 0.6141 - accuracy: 0.7817\n",
      "Epoch 99/100\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 0.5913 - accuracy: 0.7936\n",
      "Epoch 100/100\n",
      "27/27 [==============================] - 11s 396ms/step - loss: 0.5985 - accuracy: 0.7945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef0c8f1600>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y , batch_size=128, epochs=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is used as a metric, it seems to be at 79%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate New Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello sorr online tim leg ever\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"hello\"\n",
    "next_words = 5\n",
    "for _ in range(next_words):\n",
    "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre', truncating='pre')\n",
    "    y_pred = model.predict(encoded, verbose=0)\n",
    "    predicted_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == np.argmax(y_pred):\n",
    "            predicted_word = word\n",
    "            break\n",
    "    seed_text += \" \" + predicted_word\n",
    "\n",
    "print(seed_text)\n",
    "# print(df1.text[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
